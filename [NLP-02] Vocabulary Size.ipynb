{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a27b4ed",
   "metadata": {},
   "source": [
    "데이터의 전처리는 모델의 성능에 직접적인 영향을 준다.  \n",
    "특히 BoW를 기반으로 하는 DTM 이나 TF-IDF 의 경우, 사용하는 **단어의 수** 를 어떻게 결정하느냐에 따라서 성능에 영향을 줄 수 있다.\n",
    "\n",
    "단어의 수는 또 어떤 모델을 사용하느냐에 따라 유리할 수도, 불리할 수도 있다.\n",
    "\n",
    "단어의 수에 따라 모델의 성능이 어떻게 변하는지 테스트해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290e800",
   "metadata": {},
   "source": [
    "### 라이브러리 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1cc1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "3.4.3\n",
      "0.11.2\n",
      "1.22.2\n",
      "1.3.3\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import matplotlib\n",
    "import seaborn \n",
    "import numpy \n",
    "import pandas\n",
    "import sklearn\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "print(matplotlib.__version__)\n",
    "print(seaborn.__version__)\n",
    "print(numpy.__version__)\n",
    "print(pandas.__version__)\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd83e5d",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기\n",
    "사용할 데이터는 로이터 뉴스 데이터이다. 총 46개의 클래스로 구성되며, 해당 뉴스가 어느 카테고리에 속하는지를 예측하기 위한 데이터이다.   \n",
    "텐서플로우 데이터셋에서 제공하고 있는 데이터로 쉽게 다운로드 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7884649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb748b2",
   "metadata": {},
   "source": [
    "### (1) 훈련데이터와 테스트 데이터 로드하기\n",
    "총 세 가지 경우의 단어 개수를 가지고 실험한다.\n",
    "1) 모든 단어 사용  \n",
    "2) 빈도수 상위 5,000개의 단어만 사용  \n",
    "3) 직접 단어 개수 설정해서 사용\n",
    "\n",
    "# 1) 모든 단어 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1463b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c1fff43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플의 수: 8982\n",
      "테스트 샘플의 수: 2246\n"
     ]
    }
   ],
   "source": [
    "# 데이터 구성 확인\n",
    "print('훈련 샘플의 수: {}'.format(len(x_train)))\n",
    "print('테스트 샘플의 수: {}'.format(len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b77cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스의 수: 46\n"
     ]
    }
   ],
   "source": [
    "# 클래스 개수 \n",
    "num_classes = max(y_train) + 1\n",
    "print('클래스의 수: {}'.format(num_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15796ab",
   "metadata": {},
   "source": [
    "### (2) 데이터 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f984b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련용 뉴스의 최대 길이 : 2376\n",
      "훈련용 뉴스의 평균 길이 : 145.5398574927633\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('훈련용 뉴스의 최대 길이 : {}'.format(max(len(l) for l in x_train)))\n",
    "print('훈련용 뉴스의 평균 길이 : {}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "      \n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d06de00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 클래스 별로 확인\n",
    "fig, ax = plt.subplots(ncols=1)\n",
    "fig.set_size_inches(11, 5)\n",
    "sns.countplot(x=y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cafa24",
   "metadata": {},
   "source": [
    "3번, 4번 클래스가 대부분을 차지하고 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c8d81",
   "metadata": {},
   "source": [
    "## 2. 데이터 복원하기\n",
    "### 원본 뉴스 데이터로 복원\n",
    "이 데이터는 이미 어느 정도 전처리가 되어 각 단어가 정수 시퀀스로 변환된 채 제공되지만, 이를 텍스트로 다시 돌려 벡터화를 시킨다. \n",
    "\n",
    "아래와 같이 로이터 뉴스 데이터는 '단어'를 key값으로, 고유한 '정수'를 value로 가지는 dictionary를 제공한다. 이를 word_index로 저장해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce493d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index(path='reuters_word_index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4682139e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225ddd37",
   "metadata": {},
   "source": [
    "뉴스 데이터 안에서 단어 'the'는 사실 1번이 아니라 4번 단어이다. \n",
    "\n",
    "0번, 1번, 2번은 사실 각각 <pad\\>, <sos\\>, <unk\\>라는 자연어 처리를 위한 특별한 토큰들을 위해 맵핑된 번호이기 때문에 word_index에서 index_word를 만들 때, 각 정수에 +3을 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48e98253",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = {index+3 : word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757bc440",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b9ea738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_to_word 에 숫자 0은 <pad>, 1은 <sos>, 2는 <unk>\n",
    "for index, token in enumerate(('<pad>','<sos>','<unk>')):\n",
    "    index_to_word[index] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a97e33bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos>\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43b7a2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 훈련용 뉴스 기사를 원래 텍스트로 복원해본다.\n",
    "print(' '.join([index_to_word[index] for index in x_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2ad06d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련용 뉴스 데이터와 테스트용 뉴스 데이터를 텍스트 데이터로 변환\n",
    "# 훈련용 데이터\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "    \n",
    "x_train = decoded\n",
    "print(len(x_train))\n",
    "\n",
    "# 테스트용 데이터\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "    \n",
    "x_test = decoded\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65107708",
   "metadata": {},
   "source": [
    "## 3. 벡터화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a85330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8ed409",
   "metadata": {},
   "source": [
    "### (1) DTM 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ade7789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print(x_train_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6af8b8",
   "metadata": {},
   "source": [
    "26506개의 단어가 사용됨을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633108dd",
   "metadata": {},
   "source": [
    "### (2) TF-IDF Matrix 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b04085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 26506)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "print(tfidfv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d99fc7",
   "metadata": {},
   "source": [
    "## 4. 모델 적용하기\n",
    ">**사용할 모델**  \n",
    "나이브 베이즈 분류기, CNB, 로지스틱 회귀, 서포트 벡터 머신, 결정 트리, 랜덤 포레스트, 그래디언트 부스팅 트리, 보팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "331e9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 불러오기\n",
    "from sklearn.naive_bayes import MultinomialNB # 다항분포 나이브베이즈\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score # 정확도 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aaf249",
   "metadata": {},
   "source": [
    "### (1) 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94895d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.4301           19.45m\n",
      "         2       76760.8864           19.24m\n",
      "         3   766490025.2967           19.00m\n",
      "         4 660857139232122368.0000           18.75m\n",
      "         5 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           18.60m\n",
      "         6 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           18.46m\n",
      "         7 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           18.24m\n",
      "         8 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           18.03m\n",
      "         9 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           17.82m\n",
      "        10 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           17.63m\n",
      "        11 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           17.45m\n",
      "        12 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           17.24m\n",
      "        13 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           17.03m\n",
      "        14 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           16.82m\n",
      "        15 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           16.61m\n",
      "        16 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           16.41m\n",
      "        17 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           16.21m\n",
      "        18 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           16.01m\n",
      "        19 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           15.80m\n",
      "        20 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           15.60m\n",
      "        21 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           15.41m\n",
      "        22 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           15.23m\n",
      "        23 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           15.03m\n",
      "        24 1006529060407114004275646732312951615311370376755613476759245377256674837692003341988813591601665584394912965173495998495951159296.0000           14.84m\n",
      "        25 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           14.65m\n",
      "        26 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           14.45m\n",
      "        27 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           14.26m\n",
      "        28 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           14.07m\n",
      "        29 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           13.88m\n",
      "        30 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           13.68m\n",
      "        31 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           13.50m\n",
      "        32 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           13.31m\n",
      "        33 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           13.11m\n",
      "        34 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           12.92m\n",
      "        35 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           12.73m\n",
      "        36 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           12.54m\n",
      "        37 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           12.36m\n",
      "        38 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           12.19m\n",
      "        39 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           12.00m\n",
      "        40 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.80m\n",
      "        41 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.61m\n",
      "        42 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.41m\n",
      "        43 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.22m\n",
      "        44 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.02m\n",
      "        45 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.82m\n",
      "        46 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.63m\n",
      "        47 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.43m\n",
      "        48 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.24m\n",
      "        49 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           10.04m\n",
      "        50 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.84m\n",
      "        51 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.65m\n",
      "        52 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.45m\n",
      "        53 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.25m\n",
      "        54 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            9.06m\n",
      "        55 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.87m\n",
      "        56 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.67m\n",
      "        57 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.47m\n",
      "        58 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.27m\n",
      "        59 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            8.08m\n",
      "        60 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.88m\n",
      "        61 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.68m\n",
      "        62 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.49m\n",
      "        63 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.29m\n",
      "        64 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            7.09m\n",
      "        65 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.89m\n",
      "        66 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.70m\n",
      "        67 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.50m\n",
      "        68 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.30m\n",
      "        69 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            6.11m\n",
      "        70 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.91m\n",
      "        71 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.71m\n",
      "        72 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.51m\n",
      "        73 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.31m\n",
      "        74 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            5.12m\n",
      "        75 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.92m\n",
      "        76 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.72m\n",
      "        77 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.53m\n",
      "        78 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.33m\n",
      "        79 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            4.13m\n",
      "        80 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.93m\n",
      "        81 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.74m\n",
      "        82 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.54m\n",
      "        83 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.34m\n",
      "        84 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            3.15m\n",
      "        85 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.95m\n",
      "        86 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.75m\n",
      "        87 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.56m\n",
      "        88 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.36m\n",
      "        89 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            2.16m\n",
      "        90 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.97m\n",
      "        91 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.77m\n",
      "        92 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.57m\n",
      "        93 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.38m\n",
      "        94 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            1.18m\n",
      "        95 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           58.97s\n",
      "        96 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           47.18s\n",
      "        97 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           35.38s\n",
      "        98 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           23.59s\n",
      "        99 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000           11.79s\n",
      "       100 7935486330861459413268209935908863185564832687635547920920276400843855648137909550861590845608894276096734983386906163942311234165539923296256.0000            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=10000)),\n",
       "                             ('cnb', ComplementNB()),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(random_state=0))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "nb = MultinomialNB()\n",
    "cnb = ComplementNB()\n",
    "lr = LogisticRegression(C = 10000, penalty='l2', max_iter=10000)\n",
    "lsvc = LinearSVC(C = 1000, penalty = 'l1', max_iter = 10000, dual=False)\n",
    "tree = DecisionTreeClassifier(max_depth = 10, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators = 5, random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0, verbose=3)\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "    ('cnb', ComplementNB()),\n",
    "    ('gb', GradientBoostingClassifier(random_state=0)),\n",
    "], voting='soft', n_jobs=-1)\n",
    "\n",
    "# 학습\n",
    "nb.fit(tfidfv, y_train)\n",
    "cnb.fit(tfidfv, y_train)\n",
    "lr.fit(tfidfv, y_train)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "tree.fit(tfidfv, y_train)\n",
    "forest.fit(tfidfv, y_train)\n",
    "gb.fit(tfidfv, y_train)\n",
    "voting.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c0f6c6",
   "metadata": {},
   "source": [
    "### (2) 테스트 데이터 변환\n",
    "훈련 데이터와 동일한 전처리를 수행하여 정확도를 측정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d03ce5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 DTM 으로 변환\n",
    "x_test_dtm = dtmvector.transform(x_test)\n",
    "\n",
    "# DTM을 tfidf로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac17c7c",
   "metadata": {},
   "source": [
    "### (3) 예측 및 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "236a87c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB acc: 0.59973\n",
      "ComplementNB  acc: 0.76492\n",
      "LogisticRegression acc: 0.81745\n",
      "    SVM      acc: 0.78718\n",
      "DecisionTree acc: 0.62110\n",
      "RandomForest acc: 0.65450\n",
      "GradientBoosting acc: 0.77026\n",
      "   Voting    acc: 0.81879\n"
     ]
    }
   ],
   "source": [
    "# 예측값\n",
    "nb_pred = nb.predict(tfidfv_test)\n",
    "cnb_pred = cnb.predict(tfidfv_test)\n",
    "lr_pred = lr.predict(tfidfv_test)\n",
    "lsvc_pred = lsvc.predict(tfidfv_test)\n",
    "tree_pred = tree.predict(tfidfv_test)\n",
    "forest_pred = forest.predict(tfidfv_test)\n",
    "gb_pred = gb.predict(tfidfv_test)\n",
    "voting_pred = voting.predict(tfidfv_test)\n",
    "\n",
    "# 정확도 측정\n",
    "print('MultinomialNB acc: {:.5f}'.format(accuracy_score(y_test, nb_pred)))\n",
    "print('ComplementNB  acc: {:.5f}'.format(accuracy_score(y_test, cnb_pred)))\n",
    "print('LogisticRegression acc: {:.5f}'.format(accuracy_score(y_test, lr_pred)))\n",
    "print('    SVM      acc: {:.5f}'.format(accuracy_score(y_test, lsvc_pred)))\n",
    "print('DecisionTree acc: {:.5f}'.format(accuracy_score(y_test, tree_pred)))\n",
    "print('RandomForest acc: {:.5f}'.format(accuracy_score(y_test, forest_pred)))\n",
    "print('GradientBoosting acc: {:.5f}'.format(accuracy_score(y_test, gb_pred)))\n",
    "print('   Voting    acc: {:.5f}'.format(accuracy_score(y_test, voting_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9f374f",
   "metadata": {},
   "source": [
    "전체 단어 수를 이용해 문서를 분류한 결과,  \n",
    "voting > Logistic > SVM > GradientBoosting > ComplementNB 순으로 정확도가 높았다.    \n",
    "아무래도 순위권에 있는 logistic, gradientboosting, complementNB 가 voting에서 앙상블로 쓰이다 보니 voting의 성능이 가장 높게 나온 것 같다.  \n",
    "ComplementNB의 경우 MultinomialNB와 비교했을 때 성능 차이가 많이 나는데, 아무래도 뉴스데이터의 label 불균형 때문에 불균형한 경우에 대비한 ComplementNB가 훨씬 좋은 성능을 보인 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7052e367",
   "metadata": {},
   "source": [
    "# 2) 빈도수 상위 5,000개의 단어만 사용\n",
    "### 1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd51f0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb081598",
   "metadata": {},
   "source": [
    "### 2 데이터 복원하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7083f542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path='reuters_word_index.json')\n",
    "\n",
    "index_to_word = {index+3 : word for word, index in word_index.items()}\n",
    "for index, token in enumerate(('<pad>','<sos>','<unk>')):\n",
    "    index_to_word[index] = token\n",
    "    \n",
    "# 전체 훈련용 뉴스 데이터와 테스트용 뉴스 데이터를 텍스트 데이터로 변환\n",
    "# 훈련용 데이터\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "    \n",
    "x_train = decoded\n",
    "print(len(x_train))\n",
    "\n",
    "# 테스트용 데이터\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "    \n",
    "x_test = decoded\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50babec5",
   "metadata": {},
   "source": [
    "### 3 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a7b1de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM shape (8982, 4867)\n",
      "TFIDF (8982, 4867)\n"
     ]
    }
   ],
   "source": [
    "# DTM 생성\n",
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print('DTM shape',x_train_dtm.shape)\n",
    "\n",
    "# TF-IDF Matrix\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "print('TFIDF', tfidfv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1e011",
   "metadata": {},
   "source": [
    "### 4 모델 적용 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cf9eba7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.4697           16.74m\n",
      "         2     2131099.0239           16.82m\n",
      "         3 113352934366748750033493137947714414201794552363528671409179356940992512.0000           16.74m\n",
      "         4 645005367195105573487290209089081189090170195029150070319832395667206596719018218881024.0000           16.60m\n",
      "         5 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           16.44m\n",
      "         6 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           16.29m\n",
      "         7 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           16.11m\n",
      "         8 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           15.94m\n",
      "         9 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           15.78m\n",
      "        10 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           15.64m\n",
      "        11 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           15.47m\n",
      "        12 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           15.31m\n",
      "        13 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           15.14m\n",
      "        14 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           14.97m\n",
      "        15 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           14.80m\n",
      "        16 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           14.63m\n",
      "        17 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           14.45m\n",
      "        18 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           14.28m\n",
      "        19 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           14.10m\n",
      "        20 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.93m\n",
      "        21 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.76m\n",
      "        22 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.59m\n",
      "        23 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.41m\n",
      "        24 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.24m\n",
      "        25 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           13.06m\n",
      "        26 443067299530301659980374363525949995629695549691559890285869350923592723545213280554158712204202745088665306932510573934053386280801140736.0000           12.89m\n",
      "        27 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           12.71m\n",
      "        28 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           12.53m\n",
      "        29 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           12.36m\n",
      "        30 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           12.18m\n",
      "        31 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           12.01m\n",
      "        32 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           11.83m\n",
      "        33 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           11.66m\n",
      "        34 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           11.48m\n",
      "        35 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           11.31m\n",
      "        36 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           11.14m\n",
      "        37 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.96m\n",
      "        38 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.79m\n",
      "        39 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.61m\n",
      "        40 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.43m\n",
      "        41 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.26m\n",
      "        42 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.08m\n",
      "        43 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.90m\n",
      "        44 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.73m\n",
      "        45 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.55m\n",
      "        46 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.38m\n",
      "        47 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.20m\n",
      "        48 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            9.03m\n",
      "        49 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.86m\n",
      "        50 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.68m\n",
      "        51 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.51m\n",
      "        52 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.33m\n",
      "        53 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            8.16m\n",
      "        54 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.98m\n",
      "        55 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.81m\n",
      "        56 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.63m\n",
      "        57 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.46m\n",
      "        58 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.28m\n",
      "        59 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            7.11m\n",
      "        60 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.94m\n",
      "        61 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.76m\n",
      "        62 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.59m\n",
      "        63 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.42m\n",
      "        64 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.24m\n",
      "        65 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            6.07m\n",
      "        66 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.89m\n",
      "        67 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.72m\n",
      "        68 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.55m\n",
      "        69 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.37m\n",
      "        70 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.20m\n",
      "        71 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            5.02m\n",
      "        72 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.85m\n",
      "        73 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.68m\n",
      "        74 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.50m\n",
      "        75 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.33m\n",
      "        76 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            4.16m\n",
      "        77 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.98m\n",
      "        78 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.81m\n",
      "        79 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.64m\n",
      "        80 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.46m\n",
      "        81 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.29m\n",
      "        82 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            3.12m\n",
      "        83 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.94m\n",
      "        84 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.77m\n",
      "        85 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.60m\n",
      "        86 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.42m\n",
      "        87 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.25m\n",
      "        88 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            2.08m\n",
      "        89 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.90m\n",
      "        90 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.73m\n",
      "        91 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.56m\n",
      "        92 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.38m\n",
      "        93 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.21m\n",
      "        94 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            1.04m\n",
      "        95 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           51.89s\n",
      "        96 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           41.51s\n",
      "        97 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           31.13s\n",
      "        98 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           20.75s\n",
      "        99 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000           10.37s\n",
      "       100 443067299530302073140354857431324340576763070173459783038796203599354776450710327057778234898793885831921831754563603678560901920400080896.0000            0.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=10000)),\n",
       "                             ('cnb', ComplementNB()),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(random_state=0))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 생성\n",
    "'''\n",
    "nb = MultinomialNB()\n",
    "cnb = ComplementNB()\n",
    "lr = LogisticRegression(C = 10000, penalty='l2', max_iter=10000)\n",
    "lsvc = LinearSVC(C = 1000, penalty = 'l1', max_iter = 10000, dual=False)\n",
    "tree = DecisionTreeClassifier(max_depth = 10, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators = 5, random_state=0)\n",
    "gb = GradientBoostingClassifier(random_state=0)#, verbose=3)\n",
    "voting = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "    ('cnb', ComplementNB()),\n",
    "    ('gb', GradientBoostingClassifier(random_state=0)),\n",
    "], voting='soft', n_jobs=-1)\n",
    "'''\n",
    "# 학습\n",
    "nb.fit(tfidfv, y_train)\n",
    "cnb.fit(tfidfv, y_train)\n",
    "lr.fit(tfidfv, y_train)\n",
    "lsvc.fit(tfidfv, y_train)\n",
    "tree.fit(tfidfv, y_train)\n",
    "forest.fit(tfidfv, y_train)\n",
    "gb.fit(tfidfv, y_train)\n",
    "voting.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b07a22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 DTM 으로 변환\n",
    "x_test_dtm = dtmvector.transform(x_test)\n",
    "\n",
    "# DTM을 tfidf로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f7a9a5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도 수 상위 5,000개 단어\n",
      "MultinomialNB acc: 0.67320\n",
      "ComplementNB  acc: 0.77070\n",
      "LogisticRegression acc: 0.80276\n",
      "    SVM      acc: 0.76803\n",
      "DecisionTree acc: 0.61799\n",
      "RandomForest acc: 0.70125\n",
      "GradientBoosting acc: 0.76759\n",
      "   Voting    acc: 0.81612\n"
     ]
    }
   ],
   "source": [
    "# 예측값\n",
    "nb_pred = nb.predict(tfidfv_test)\n",
    "cnb_pred = cnb.predict(tfidfv_test)\n",
    "lr_pred = lr.predict(tfidfv_test)\n",
    "lsvc_pred = lsvc.predict(tfidfv_test)\n",
    "tree_pred = tree.predict(tfidfv_test)\n",
    "forest_pred = forest.predict(tfidfv_test)\n",
    "gb_pred = gb.predict(tfidfv_test)\n",
    "voting_pred = voting.predict(tfidfv_test)\n",
    "\n",
    "# 정확도 측정\n",
    "print('빈도 수 상위 5,000개 단어')\n",
    "print('MultinomialNB acc: {:.5f}'.format(accuracy_score(y_test, nb_pred)))\n",
    "print('ComplementNB  acc: {:.5f}'.format(accuracy_score(y_test, cnb_pred)))\n",
    "print('LogisticRegression acc: {:.5f}'.format(accuracy_score(y_test, lr_pred)))\n",
    "print('    SVM      acc: {:.5f}'.format(accuracy_score(y_test, lsvc_pred)))\n",
    "print('DecisionTree acc: {:.5f}'.format(accuracy_score(y_test, tree_pred)))\n",
    "print('RandomForest acc: {:.5f}'.format(accuracy_score(y_test, forest_pred)))\n",
    "print('GradientBoosting acc: {:.5f}'.format(accuracy_score(y_test, gb_pred)))\n",
    "print('   Voting    acc: {:.5f}'.format(accuracy_score(y_test, voting_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea0f57",
   "metadata": {},
   "source": [
    "빈도수 상위 5,000개 단어에 대해 문서를 분류해보았다.  \n",
    "Voting > Logistic > Complement > SVM > GradientBoosting 순으로 높은 정확도를 보인다.  \n",
    "전체 단어를 사용한 경우와 비교했을 때, voting은 역시나 가장 높은 성능을 보였고, ComplementNB가 svm이나 부스팅보다 높은 정확도를 보였다. (차이는 0.01 정도로 큰 차이는 아니다.)\n",
    "\n",
    "정확도뿐만 아니라 조화평균(f1_score)은 어떻게 나는지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1efe0ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB f1_score: 0.60125\n",
      "ComplementNB f1_score: 0.74590\n",
      "Logistic Regression f1: 0.79735\n",
      "    SVM     f1_score: 0.76530\n",
      "DecisionTree f1_score: 0.57300\n",
      "RandomForest f1_score: 0.67702\n",
      "GradientBoosting f1_score: 0.76625\n",
      "   Voting    f1_score: 0.81267\n"
     ]
    }
   ],
   "source": [
    "# f1 score 측정\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# f1_score(y_true, y_pred, average=['weighted'])\n",
    "#average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택]\n",
    "\n",
    "print('MultinomialNB f1_score: {:.5f}'.format(f1_score(y_test, nb_pred, average='weighted')))\n",
    "print('ComplementNB f1_score: {:.5f}'.format(f1_score(y_test, cnb_pred, average='weighted')))\n",
    "print('Logistic Regression f1: {:.5f}'.format(f1_score(y_test, lr_pred, average='weighted')))\n",
    "print('    SVM     f1_score: {:.5f}'.format(f1_score(y_test, lsvc_pred, average='weighted')))\n",
    "print('DecisionTree f1_score: {:.5f}'.format(f1_score(y_test, tree_pred, average='weighted')))\n",
    "print('RandomForest f1_score: {:.5f}'.format(f1_score(y_test, forest_pred, average='weighted')))\n",
    "print('GradientBoosting f1_score: {:.5f}'.format(f1_score(y_test, gb_pred, average='weighted')))\n",
    "print('   Voting    f1_score: {:.5f}'.format(f1_score(y_test, voting_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd74a0",
   "metadata": {},
   "source": [
    "f1_score는 \n",
    "Voting > Logistic > Gradientboosting > svm > ComplementNB 순이다.  \n",
    "정확도 순위와는 차이가 있지만 그럼에도 Voting과 Logistic Regression은 높은 수치를 보인다. \n",
    "\n",
    "DecisionTree는 정확도나 f1_score나 가장 낮은 성능을 보이는데, 아마 훈련데이터에 대한 과적합때문이 아닐까."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e378d",
   "metadata": {},
   "source": [
    "# 3) 직접 단어 개수 설정 : 1000개\n",
    "5000, 10000, 20000  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c5d8c",
   "metadata": {},
   "source": [
    "### 1 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40b0befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=1000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dd2d5a",
   "metadata": {},
   "source": [
    "### 2 데이터 복원하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "073f5694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8982\n",
      "2246\n"
     ]
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path='reuters_word_index.json')\n",
    "\n",
    "index_to_word = {index+3 : word for word, index in word_index.items()}\n",
    "for index, token in enumerate(('<pad>','<sos>','<unk>')):\n",
    "    index_to_word[index] = token\n",
    "    \n",
    "# 전체 훈련용 뉴스 데이터와 테스트용 뉴스 데이터를 텍스트 데이터로 변환\n",
    "# 훈련용 데이터\n",
    "decoded = []\n",
    "for i in range(len(x_train)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "    decoded.append(t)\n",
    "    \n",
    "x_train = decoded\n",
    "print(len(x_train))\n",
    "\n",
    "# 테스트용 데이터\n",
    "decoded = []\n",
    "for i in range(len(x_test)):\n",
    "    t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "    decoded.append(t)\n",
    "    \n",
    "x_test = decoded\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3365749",
   "metadata": {},
   "source": [
    "### 3 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e2efb23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTM shape (8982, 969)\n",
      "TFIDF (8982, 969)\n"
     ]
    }
   ],
   "source": [
    "# DTM 생성\n",
    "dtmvector = CountVectorizer()\n",
    "x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "print('DTM shape',x_train_dtm.shape)\n",
    "\n",
    "# TF-IDF Matrix\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "print('TFIDF', tfidfv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25418de",
   "metadata": {},
   "source": [
    "### 4 모델 적용 및 평가\n",
    "#### 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b59d210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28f192e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB()"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnb.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b84c1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10000, max_iter=10000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d0be6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1199: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1000, dual=False, max_iter=10000, penalty='l1')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e253c0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_depth=10, random_state=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "453ffdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=5, random_state=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54401223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.5717           13.58m\n",
      "         2        3967.8426           13.58m\n",
      "         3        4754.6181           13.50m\n",
      "         4     1016060.1416           13.37m\n",
      "         5     1016079.3289           13.26m\n",
      "         6     6974797.9520           13.13m\n",
      "         7     6974845.8927           12.99m\n",
      "         8     6974845.8761           12.85m\n",
      "         9 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           12.71m\n",
      "        10 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           12.57m\n",
      "        11 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           12.44m\n",
      "        12 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           12.30m\n",
      "        13 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           12.16m\n",
      "        14 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           12.02m\n",
      "        15 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           11.88m\n",
      "        16 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           11.74m\n",
      "        17 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           11.60m\n",
      "        18 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           11.46m\n",
      "        19 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           11.32m\n",
      "        20 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           11.17m\n",
      "        21 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           11.03m\n",
      "        22 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.89m\n",
      "        23 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.75m\n",
      "        24 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.61m\n",
      "        25 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.47m\n",
      "        26 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.33m\n",
      "        27 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.19m\n",
      "        28 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000           10.05m\n",
      "        29 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.91m\n",
      "        30 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.78m\n",
      "        31 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.64m\n",
      "        32 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.50m\n",
      "        33 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.36m\n",
      "        34 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.22m\n",
      "        35 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            9.08m\n",
      "        36 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.94m\n",
      "        37 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.80m\n",
      "        38 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.66m\n",
      "        39 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.52m\n",
      "        40 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.38m\n",
      "        41 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.24m\n",
      "        42 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            8.10m\n",
      "        43 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.96m\n",
      "        44 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.82m\n",
      "        45 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.68m\n",
      "        46 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.54m\n",
      "        47 67711611000699444040275566464186667477192234919406304097104172655300731568078356217856.0000            7.40m\n",
      "        48 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            7.26m\n",
      "        49 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            7.12m\n",
      "        50 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.98m\n",
      "        51 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.84m\n",
      "        52 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.70m\n",
      "        53 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.56m\n",
      "        54 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.42m\n",
      "        55 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.28m\n",
      "        56 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.14m\n",
      "        57 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            6.00m\n",
      "        58 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.86m\n",
      "        59 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.72m\n",
      "        60 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.58m\n",
      "        61 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.44m\n",
      "        62 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.30m\n",
      "        63 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.16m\n",
      "        64 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            5.02m\n",
      "        65 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.89m\n",
      "        66 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.75m\n",
      "        67 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.61m\n",
      "        68 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.47m\n",
      "        69 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.33m\n",
      "        70 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.19m\n",
      "        71 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            4.05m\n",
      "        72 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.91m\n",
      "        73 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.77m\n",
      "        74 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.63m\n",
      "        75 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.49m\n",
      "        76 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.35m\n",
      "        77 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.21m\n",
      "        78 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            3.07m\n",
      "        79 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.93m\n",
      "        80 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.79m\n",
      "        81 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.65m\n",
      "        82 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.51m\n",
      "        83 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.37m\n",
      "        84 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.23m\n",
      "        85 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            2.10m\n",
      "        86 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.96m\n",
      "        87 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.82m\n",
      "        88 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.68m\n",
      "        89 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.54m\n",
      "        90 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.40m\n",
      "        91 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.26m\n",
      "        92 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            1.12m\n",
      "        93 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           58.67s\n",
      "        94 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           50.29s\n",
      "        95 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           41.91s\n",
      "        96 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           33.53s\n",
      "        97 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           25.14s\n",
      "        98 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000           16.76s\n",
      "        99 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            8.38s\n",
      "       100 303698365757899151117930826970160643457586921047431169716899952031599760807687627670376264625793489960960.0000            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(random_state=0, verbose=3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f2e917f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(C=10000)),\n",
       "                             ('cnb', ComplementNB()),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(random_state=0))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting.fit(tfidfv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5618d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터를 DTM 으로 변환\n",
    "x_test_dtm = dtmvector.transform(x_test)\n",
    "\n",
    "# DTM을 tfidf로 변환\n",
    "tfidfv_test = tfidf_transformer.transform(x_test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e32bb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈도 수 상위 1,000개 단어\n",
      "MultinomialNB acc: 0.68566\n",
      "ComplementNB  acc: 0.73464\n",
      "LogisticRegression acc: 0.73865\n",
      "    SVM      acc: 0.72262\n",
      "DecisionTree acc: 0.61799\n",
      "RandomForest acc: 0.70748\n",
      "GradientBoosting acc: 0.74533\n",
      "   Voting    acc: 0.78406\n"
     ]
    }
   ],
   "source": [
    "# 예측값\n",
    "nb_pred = nb.predict(tfidfv_test)\n",
    "cnb_pred = cnb.predict(tfidfv_test)\n",
    "lr_pred = lr.predict(tfidfv_test)\n",
    "lsvc_pred = lsvc.predict(tfidfv_test)\n",
    "tree_pred = tree.predict(tfidfv_test)\n",
    "forest_pred = forest.predict(tfidfv_test)\n",
    "gb_pred = gb.predict(tfidfv_test)\n",
    "voting_pred = voting.predict(tfidfv_test)\n",
    "\n",
    "# 정확도 측정\n",
    "print('빈도 수 상위 1,000개 단어')\n",
    "print('MultinomialNB acc: {:.5f}'.format(accuracy_score(y_test, nb_pred)))\n",
    "print('ComplementNB  acc: {:.5f}'.format(accuracy_score(y_test, cnb_pred)))\n",
    "print('LogisticRegression acc: {:.5f}'.format(accuracy_score(y_test, lr_pred)))\n",
    "print('    SVM      acc: {:.5f}'.format(accuracy_score(y_test, lsvc_pred)))\n",
    "print('DecisionTree acc: {:.5f}'.format(accuracy_score(y_test, tree_pred)))\n",
    "print('RandomForest acc: {:.5f}'.format(accuracy_score(y_test, forest_pred)))\n",
    "print('GradientBoosting acc: {:.5f}'.format(accuracy_score(y_test, gb_pred)))\n",
    "print('   Voting    acc: {:.5f}'.format(accuracy_score(y_test, voting_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe081b",
   "metadata": {},
   "source": [
    "빈도수 상위 5000보다 좋은 결과면 500도 해보려고 했지만 안 해도 될 것 같다. \n",
    "\n",
    "MultinomialNB의 경우 0.01 정도의 성능향상이 있었지만 나머지 모델에서 이보다 큰 성능 저하가 있다.  \n",
    "역시나 voting의 성능이 가장 높다.\n",
    "\n",
    "루브릭에서 분류 모델의 f1 score가 기준 이상 높게 나왔냐는 질문을 이 모델들을 돌리고 나서야 보게 되어 이번 단어 개수에 대한 성능이 높게 나오길 바랬는데 아쉽게 되었다. \n",
    "\n",
    "모델들 돌리는 데 생각보다 많은 시간이 걸린다.. max_iter 때문에 그런 것 같다. gradient boosting도 정말 오래 걸린다. + voting까지\n",
    "\n",
    "모델 변수 이름을 다 다르게 지정했으면 모델을 다시 돌리지 않고도 f1 score를 측정할 수 있었을텐데.. 앞으로는 변수 명을 꼭 다르게 지정해서 혹시 모를 상황에 대비해야겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b728a8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB f1_score: 0.63650\n",
      "ComplementNB f1_score: 0.69730\n",
      "Logistic Regression f1: 0.73243\n",
      "    SVM     f1_score: 0.71978\n",
      "DecisionTree f1_score: 0.54526\n",
      "RandomForest f1_score: 0.68285\n",
      "GradientBoosting f1_score: 0.73848\n",
      "   Voting    f1_score: 0.77702\n"
     ]
    }
   ],
   "source": [
    "# f1 score 측정\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# f1_score(y_true, y_pred, average=['weighted'])\n",
    "#average=[‘micro’, ‘macro’, ‘samples’,’weighted’ 중 하나 선택]\n",
    "\n",
    "print('MultinomialNB f1_score: {:.5f}'.format(f1_score(y_test, nb_pred, average='weighted')))\n",
    "print('ComplementNB f1_score: {:.5f}'.format(f1_score(y_test, cnb_pred, average='weighted')))\n",
    "print('Logistic Regression f1: {:.5f}'.format(f1_score(y_test, lr_pred, average='weighted')))\n",
    "print('    SVM     f1_score: {:.5f}'.format(f1_score(y_test, lsvc_pred, average='weighted')))\n",
    "print('DecisionTree f1_score: {:.5f}'.format(f1_score(y_test, tree_pred, average='weighted')))\n",
    "print('RandomForest f1_score: {:.5f}'.format(f1_score(y_test, forest_pred, average='weighted')))\n",
    "print('GradientBoosting f1_score: {:.5f}'.format(f1_score(y_test, gb_pred, average='weighted')))\n",
    "print('   Voting    f1_score: {:.5f}'.format(f1_score(y_test, voting_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8d547e",
   "metadata": {},
   "source": [
    "f1 score의 average 를 'weighted'로 주어 label의 불균형을 고려하였다.  \n",
    "voting의 정확도가 가장 높았는데, f1_score 또한 가장 높은 수치를 보이는 것으로 보아 성능이 가장 좋다고 볼 수 있을 것 같다. \n",
    "\n",
    "f1 score가 단어사전 개수에도 영향을 받는지 확인하기 위해  \n",
    "위에서 가장 성능이 좋았던 5000개 단어사전으로 학습된 모델을 다시 돌려본다.\n",
    "***\n",
    "확실히 성능에 차이가 있었다. 5000개 단어사전을 적용한 모델에서는 Voting의 f1_score가 81%로 다른 단어사전에 비해 꽤 높게 나왔다. \n",
    "\n",
    "단어사전의 개수를 총 네 가지로 분류하여 실험해보았다.  \n",
    "1) 전체 단어  \n",
    "2) 10,000 개 단어  \n",
    "3) 5,000 개 단어  \n",
    "4) 1,000 개 단어  \n",
    "\n",
    "전반적으로 성능이 가장 좋았던 단어 개수는 5,000개였다.  \n",
    "빈도수 상위 5,000개 단어사전이 정확도와 f1_score 모두 높았다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b418381",
   "metadata": {},
   "source": [
    "# 4) 딥러닝 모델과 비교해보기\n",
    "빈도수 상위 5,000개 단어사전을 가지고 데이터를 딥러닝 모델에 넣어본다.  \n",
    "모델은 RNN을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc4f41",
   "metadata": {},
   "source": [
    "### 1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baffe4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
      "2113536/2110848 [==============================] - 0s 0us/step\n",
      "2121728/2110848 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=5000, test_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371f622",
   "metadata": {},
   "source": [
    "### 2 데이터 보정 및 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fadb6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
      "557056/550378 [==============================] - 0s 0us/step\n",
      "565248/550378 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"    \\n# 전체 훈련용 뉴스 데이터와 테스트용 뉴스 데이터를 텍스트 데이터로 변환\\n# 훈련용 데이터\\ndecoded = []\\nfor i in range(len(x_train)):\\n    t = ' '.join([index_to_word[index] for index in x_train[i]])\\n    decoded.append(t)\\n    \\nx_train = decoded\\nprint(len(x_train))\\n\\n# 테스트용 데이터\\ndecoded = []\\nfor i in range(len(x_test)):\\n    t = ' '.join([index_to_word[index] for index in x_test[i]])\\n    decoded.append(t)\\n    \\nx_test = decoded\\nprint(len(x_test))\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 보정 및 복원\n",
    "word_index = reuters.get_word_index(path='reuters_word_index.json')\n",
    "\n",
    "index_to_word = {index : word for word, index in word_index.items()}\n",
    "\n",
    "#실제 인코딩 인덱스는 제공된 word_to_index에서 index 기준으로 3씩 뒤로 밀려 있습니다.  \n",
    "word_index = {k:(v+3) for k, v in word_index.items()}\n",
    "\n",
    "#처음 몇 개 인덱스는 사전에 정의되어 있다.\n",
    "word_index[\"<pad>\"] = 0\n",
    "word_index[\"<sos>\"] = 1\n",
    "word_index[\"<unk>\"] = 2 \n",
    "word_index[\"<unused>\"] = 3\n",
    "\n",
    "index_to_word = {index: word for word,index in word_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cbecda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "print(word_index['<sos>'])\n",
    "print(word_index['<pad>'])\n",
    "print(word_index['the'])\n",
    "print(index_to_word[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea9531",
   "metadata": {},
   "source": [
    "### 3 문장의 maxlen 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305cc7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 길이 평균:  145.96419665122906\n",
      "문장 길이 최대:  2376\n",
      "문장 길이 표준편차:  145.8784764459447\n",
      "pad_sequences maxlen:  437\n",
      "전체 문장의 0.9438902743142145%가 maxlen 설정값 이내에 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 텍스트데이터 문장길이의 리스트를 생성한다\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "#문장길이의 평균값, 최대값, 표준편차 계산해본다\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "print('문장 길이 평균: ', np.mean(num_tokens))\n",
    "print('문장 길이 최대: ', np.max(num_tokens))\n",
    "print('문장 길이 표준편차: ', np.std(num_tokens))\n",
    "\n",
    "# 예를 들어, 최대길이를 (평균 + 2*표준편차)로 한다면,\n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_len = int(max_tokens)\n",
    "\n",
    "print('pad_sequences maxlen: ', max_len)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다.'.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515afd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c44368",
   "metadata": {},
   "source": [
    "### 4 데이터에 pad_sequences 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb53c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ffdd212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8982, 437)\n"
     ]
    }
   ],
   "source": [
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, \n",
    "                                                      value=word_index['<pad>'],\n",
    "                                                      padding='post', #또는 'pre'\n",
    "                                                      maxlen = max_len)\n",
    "\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test,\n",
    "                                                     value=word_index['<pad>'],\n",
    "                                                     padding = 'post', #또는 'pre'\n",
    "                                                     maxlen = max_len)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96953d",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/46323296/keras-pad-sequences-throwing-invalid-literal-for-int-with-base-10\n",
    "\n",
    "keras 모델에는 string을 쓰면 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad30af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical ## 추가\n",
    "\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb00a97",
   "metadata": {},
   "source": [
    "### 5 딥러닝 모델 설계와 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42b63ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         640000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 777,518\n",
      "Trainable params: 777,518\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000 # 단어사전 크기\n",
    "word_vector_dim = 128 # 워드벡터의 차원 수\n",
    "hidden_units = 128 ## 추가\n",
    "\n",
    "# 모델 설계\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size,word_vector_dim)) ## 수정\n",
    "model.add(tf.keras.layers.LSTM(hidden_units)) ## 수정\n",
    "model.add(tf.keras.layers.Dense(46, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11da6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "71/71 [==============================] - 125s 2s/step - loss: 2.5986 - accuracy: 0.3451\n",
      "Epoch 2/20\n",
      "71/71 [==============================] - 124s 2s/step - loss: 2.4021 - accuracy: 0.3529\n",
      "Epoch 3/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3972 - accuracy: 0.3550\n",
      "Epoch 4/20\n",
      "71/71 [==============================] - 122s 2s/step - loss: 2.3959 - accuracy: 0.3550\n",
      "Epoch 5/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3967 - accuracy: 0.3549\n",
      "Epoch 6/20\n",
      "71/71 [==============================] - 122s 2s/step - loss: 2.3932 - accuracy: 0.3548\n",
      "Epoch 7/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3897 - accuracy: 0.3569\n",
      "Epoch 8/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3820 - accuracy: 0.3595\n",
      "Epoch 9/20\n",
      "71/71 [==============================] - 124s 2s/step - loss: 2.3718 - accuracy: 0.3622\n",
      "Epoch 10/20\n",
      "71/71 [==============================] - 124s 2s/step - loss: 2.3618 - accuracy: 0.3651\n",
      "Epoch 11/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3508 - accuracy: 0.3666\n",
      "Epoch 12/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3429 - accuracy: 0.3694\n",
      "Epoch 13/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3368 - accuracy: 0.3706\n",
      "Epoch 14/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3273 - accuracy: 0.3744\n",
      "Epoch 15/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.3167 - accuracy: 0.3775\n",
      "Epoch 16/20\n",
      "71/71 [==============================] - 125s 2s/step - loss: 2.3143 - accuracy: 0.3768\n",
      "Epoch 17/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.2993 - accuracy: 0.3839\n",
      "Epoch 18/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.2896 - accuracy: 0.3854\n",
      "Epoch 19/20\n",
      "71/71 [==============================] - 123s 2s/step - loss: 2.2818 - accuracy: 0.3893\n",
      "Epoch 20/20\n",
      "71/71 [==============================] - 124s 2s/step - loss: 2.2715 - accuracy: 0.3933\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(x_train, \n",
    "                   y_train,\n",
    "                   epochs = epochs,\n",
    "                   batch_size = 128,\n",
    "                   verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a27b07",
   "metadata": {},
   "source": [
    "### 6 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "685ac237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71/71 [==============================] - 17s 243ms/step - loss: 2.4373 - accuracy: 0.3722\n",
      "[2.437252998352051, 0.3722172677516937]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688f294",
   "metadata": {},
   "source": [
    "정확도가 약 37%이다.  \n",
    "아무래도 딥러닝 모델에 쓰기에는 데이터가 적은데다가 라벨도 불균형해서 모델이 학습하기 쉽지 않았을 것 같다.  \n",
    "\n",
    "결국 가장 좋은 성능을 보인 모델은 5000개 단어사전을 사용했을 때, 정확도 81.6%, f1_score 81.2를 보인 머신러닝의 Voting이었다.  \n",
    "5000개 단어사전을 사용했을 때, 대체적으로 높은 수치가 나온 모델은 voting, logistic, svm, complement, gradient boosting 이었는데, 특히 Voting에서 정확도와 f1_score 모두 높았다.   \n",
    "Voting은 앙상블 기법으로, 여러 모델의 결과물을 통해 최종값을 도출해 내는데, 이 때 쓰인 모델은 logistic, complementNB, gradient boosting 이다. 소프트맥스함수와 조건부 확률, 앙상블 기법의 결과물들을 모두 참고하다보니 높은 성능이 나오지 않았나 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8305d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
